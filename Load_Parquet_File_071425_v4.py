#!/usr/bin/env python3
# ==============================================================================
#  explore_hospital_data.py (REVISED)
#  Author: Aaron Johnson
#  Date: 2024-05-21
#
#  This script analyzes the pre-processed Parquet file generated by the
#  'geospatial_hospital_ai_robotics_analysis.py' script.
#
#  V2 Revisions:
#   - Fixes blank choropleth map by correctly mapping full state names to
#     abbreviations for merging with the shapefile.
#   - Correctly treats the 'bsc' (Bed Size Code) column as categorical data,
#     using the provided descriptions for clearer plot labels.
# ==============================================================================
import os
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import us  # <-- NEW: Library for state name/abbreviation conversion

# --- Configuration ---
BASE_DIR = Path(__file__).resolve().parent
PARQUET_FILE = BASE_DIR / "hospital_ai_robotics_enriched.parquet"
SHAPE_DIR = BASE_DIR / "shapefiles"
STATES_SHAPEFILE = SHAPE_DIR / "cb_2018_us_state_20m.shp" # Adjust filename if needed
OUTPUT_DIR = BASE_DIR / "output_figures"

# --- Main Functions ---

def load_and_prepare_data(parquet_path):
    """
    Loads hospital data from a Parquet file and converts the WKT geometry
    back into a proper GeoDataFrame.
    """
    if not parquet_path.exists():
        print(f"Error: Parquet file not found at {parquet_path}")
        return None

    print(f"Loading data from {parquet_path.name}...")
    df = pd.read_parquet(parquet_path)
    gdf = gpd.GeoDataFrame(
        df,
        geometry=gpd.GeoSeries.from_wkt(df['geometry_wkt']),
        crs="EPSG:4326"
    )
    print(f"Successfully loaded {len(gdf)} records and restored geometry.")
    return gdf

def analyze_by_state(gdf, states_shp_path):
    """
    Aggregates hospital data by state and creates a choropleth map.
    (Version 2: Correctly handles state name to abbreviation mapping)
    """
    if not states_shp_path.exists():
        print(f"Error: States shapefile not found at {states_shp_path}")
        print("Skipping state-level analysis and map generation.")
        return

    print("\n--- Starting State-Level Analysis ---")
    state_summary = gdf.groupby('state').agg(
        hospital_count=('hospital_id', 'count'),
        ai_hospitals=('ai_flag', 'sum'),
        robo_hospitals=('robo_flag', 'sum')
    ).reset_index()

    state_summary['ai_adoption_pct'] = (state_summary['ai_hospitals'] / state_summary['hospital_count']) * 100

    print("Top 10 States by AI Adoption Rate:")
    print(state_summary.sort_values('ai_adoption_pct', ascending=False).head(10).to_string(index=False))

    # === FIX FOR BLANK MAP ===
    # Convert full state names to two-letter abbreviations for the merge.
    def get_abbr(state_name):
        state_obj = us.states.lookup(state_name)
        return state_obj.abbr if state_obj else None

    state_summary['state_abbr'] = state_summary['state'].apply(get_abbr)
    state_summary.dropna(subset=['state_abbr'], inplace=True)

    print("\nGenerating choropleth map of AI adoption...")
    states_gdf = gpd.read_file(states_shp_path)

    # Merge using the NEW abbreviation column
    map_data = states_gdf.merge(state_summary, left_on='STUSPS', right_on='state_abbr', how='left')
    map_data_contig = map_data[~map_data['STUSPS'].isin(['AK', 'HI', 'PR', 'VI', 'GU', 'AS', 'MP'])]

    fig, ax = plt.subplots(1, 1, figsize=(15, 10))
    map_data_contig.plot(
        column='ai_adoption_pct',
        ax=ax,
        legend=True,
        cmap='viridis',
        linewidth=0.8,
        edgecolor='0.8',
        missing_kwds={
            "color": "lightgrey",
            "label": "No Data",
        },
        legend_kwds={'label': "Percentage of Hospitals with AI Adoption (%)",
                     'orientation': "horizontal"}
    )
    ax.set_title('State-Level AI Adoption Rate in US Hospitals', fontsize=16)
    ax.set_axis_off()

    output_path = OUTPUT_DIR / "state_ai_adoption_choropleth.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Saved state map to {output_path}")
    plt.close(fig)

def explore_correlations(gdf):
    """
    Creates a scatter plot to analyze relationships between hospital size,
    competition, and technology adoption.
    """
    print("\n--- Exploring Correlations ---")
    print("Generating scatter plot of Hospital Size vs. Market Activity...")

    # --- Data Preparation ---
    plot_df = gdf.copy()
    
    # Check what columns are available
    print(f"Available columns: {plot_df.columns.tolist()}")
    
    # Create the tech_type column based on ai_flag and robo_flag
    if 'ai_flag' in plot_df.columns and 'robo_flag' in plot_df.columns:
        print("Creating technology type categorization...")
        # Create technology adoption category
        plot_df['tech_type'] = 'Neither'
        plot_df.loc[plot_df['ai_flag'] == 1, 'tech_type'] = 'AI Only'
        plot_df.loc[plot_df['robo_flag'] == 1, 'tech_type'] = 'Robotics Only'
        plot_df.loc[(plot_df['ai_flag'] == 1) & (plot_df['robo_flag'] == 1), 'tech_type'] = 'Both AI & Robotics'
        
        # Show the distribution
        tech_counts = plot_df['tech_type'].value_counts()
        print(f"Technology adoption categories:\n{tech_counts}")
    else:
        print("Error: Required columns 'ai_flag' and/or 'robo_flag' missing from the data")
        return

    # Convert 'bsc' to numeric, forcing non-numeric values to NaN
    plot_df['bsc'] = pd.to_numeric(plot_df['bsc'], errors='coerce')
    
    # Now filter for valid bed size codes
    plot_df = plot_df[plot_df['bsc'] > 0].copy()
    
    # Create bed size descriptions
    bsc_map = {
        1: "6-24 beds", 2: "25-49 beds", 3: "50-99 beds",
        4: "100-199 beds", 5: "200-299 beds", 6: "300-399 beds",
        7: "400-499 beds", 8: "500 or more beds"
    }
    plot_df['bsc_description'] = plot_df['bsc'].map(bsc_map)
    category_order = list(bsc_map.values())
    plot_df['bsc_description'] = pd.Categorical(
        plot_df['bsc_description'], categories=category_order, ordered=True
    )
    
    # Filter to only include hospitals with some technology adoption
    plot_df_filtered = plot_df[plot_df['tech_type'] != 'Neither']
    
    # Make sure we have data to plot
    if len(plot_df_filtered) == 0:
        print("Warning: No hospitals with technology adoption found after filtering")
        return

    # --- Choose y-axis variable based on what's available ---
    # Original column 'k3_avg_miles' is not available, use an alternative
    if 'adjpd' in plot_df_filtered.columns:
        y_column = 'adjpd'
        y_label = 'Adjusted Patient Days (Log Scale)'
        print("Using 'adjpd' (Adjusted Patient Days) as a measure of hospital activity")
    elif 'ftemd' in plot_df_filtered.columns:
        y_column = 'ftemd'
        y_label = 'Full-Time Equivalent Physicians (Log Scale)'
        print("Using 'ftemd' (FTE Physicians) as a measure of hospital capacity")
    else:
        # Use a column that should definitely be available as last resort
        y_column = 'bsc'
        y_label = 'Bed Size Code'
        print("Using 'bsc' as fallback y-axis variable")
    
    # --- Plotting ---
    fig, ax = plt.subplots(figsize=(14, 8))
    sns.set_theme(style="whitegrid")
    
    # Create a placeholder column for AI intensity if it doesn't exist
    if 'ai_intensity' not in plot_df_filtered.columns:
        print("Note: 'ai_intensity' column not found, using constant size for points")
        plot_df_filtered['ai_intensity'] = 1  # Use a constant size
    
    scatter_plot = sns.scatterplot(
        data=plot_df_filtered,
        x='bsc_description', y=y_column,
        hue='tech_type', palette='viridis',
        alpha=0.7, size='ai_intensity',
        sizes=(30, 250),
        ax=ax
    )
    
    # Use log scale if using volume metrics
    if y_column in ['adjpd', 'ftemd', 'ftern']:
        plt.yscale('log')
    
    plt.title('Hospital Size, Activity, and Technology Adoption', fontsize=16)
    plt.xlabel('Hospital Size (Bed Count Range)', fontsize=12)
    plt.ylabel(y_label, fontsize=12)
    plt.xticks(rotation=45, ha='right')
    
    # Legend handling
    sns.move_legend(scatter_plot, "upper left", bbox_to_anchor=(1.02, 1))
    
    plt.text(0.98, 0.02,
             f'X-axis: Hospital bed size category\nY-axis: {y_label}',
             transform=ax.transAxes, fontsize=10,
             verticalalignment='bottom', horizontalalignment='right',
             bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))

    plt.tight_layout(rect=[0, 0, 0.85, 1])

    output_path = OUTPUT_DIR / "hospital_size_activity_tech_scatter.png"
    plt.savefig(output_path, dpi=300)
    print(f"Saved scatter plot to {output_path}")
    plt.close(fig)

def main():
    """Main function to run the analysis workflow."""
    OUTPUT_DIR.mkdir(exist_ok=True)
    hospital_gdf = load_and_prepare_data(PARQUET_FILE)
    if hospital_gdf is None:
        return
    analyze_by_state(hospital_gdf, STATES_SHAPEFILE)
    explore_correlations(hospital_gdf)
    print("\nAnalysis complete. Figures saved to 'output_figures' directory.")

if __name__ == "__main__":
    main()